# Intelligent Dispatch Monitoring System

This project is a complete, containerized solution for processing video from a kitchen dispatch area. It uses a fine-tuned YOLOv8 model to perform object detection and tracking, generating a new video file with all detections rendered.

The repository includes the final Dockerized application, as well as a full suite of scripts for local testing, development, and advanced optimization.

## Key Features & Enhancements

*   **High-Accuracy Object Tracking:** Utilizes a fine-tuned YOLOv8 model to detect and track 6 distinct classes of items (`dish_empty`, `dish_kakigori`, `dish_not_empty`, etc.).
*   **Region of Interest (ROI) Processing:** All detection and tracking is focused on a specific, pre-defined counter area, making the system highly efficient and reducing irrelevant detections.
*   **Kalman Filter Smoothing:** Implements Kalman Filters on bounding box coordinates to produce exceptionally smooth, stable tracking and reduce jitter from frame-to-frame detection noise.
*   **Dockerized Batch Processing:** The final application is containerized using Docker and managed with Docker Compose for easy, one-command execution, ensuring consistent results in any environment.
*   **Interactive Web UI for Testing:** A local testing interface built with Streamlit (`src/app.py`) allows for real-time visualization and debugging.
*   **User Feedback Mechanism:** The web UI includes a "Flag Frame" feature to save frames for model improvement, demonstrating a complete MLOps active learning loop concept.
*   **Efficient Development Workflow:** Includes dedicated scripts (`split.py`, `test.py`) to automatically create shorter test clips from the source video, enabling rapid testing and iteration.
*   **Reinforcement Learning for Optimization:** Includes a conceptual script (`optimize_threshold_rl.py`) that uses a Q-Learning agent to autonomously find the optimal confidence threshold for the model.

## Technology Stack

*   **Model:** YOLOv8 (fine-tuned with PyTorch)
*   **Core Libraries:** OpenCV, PyTorch, Ultralytics, NumPy, Streamlit
*   **Deployment:** Docker & Docker Compose

---

## Project Structure

The project is organized into distinct directories for configuration, models, and source code, following best practices for maintainability.

```
Intelligent_Monitor_System/
├── config/
│   └── data.yaml           # YOLO data configuration for training.
├── models/
│   └── best.pt             # The final, trained PyTorch model.
├── src/
│   ├── app.py              # The Streamlit web app for interactive local testing.
│   └── process_video.py    # The main batch-processing script run by Docker.
├── Dataset/                  # (Ignored by Git) Contains all training & annotation data.
├── video_clips/              # (Ignored by Git) Auto-generated by split.py for testing.
├── feedback_inbox/           # (Ignored by Git) For flagged frames from the web app.
├── dispatch_video.mp4        # The full-length input video file.
├── test.py                   # Local script to run batch processing on a short clip.
├── split.py                  # Script to create a shorter video clip for testing.
├── optimize_threshold_rl.py  # Script to find the optimal confidence threshold.
├── train.py                  # Script used in Colab to train the model.
├── Dockerfile                # Defines the Docker container environment.
├── docker-compose.yml        # Manages the Docker service for easy execution.
├── requirements.txt          # Python dependencies for the local environment.
└── README.md                 # This file.
```

---

## Final Execution with Docker (Main Submission)

This is the primary method for running the application. It builds a container and runs the batch process to generate the final annotated video.

### Prerequisites

*   [Docker](https://www.docker.com/get-started) and Docker Compose must be installed and running.
*   [Git](https://git-scm.com/) installed on your system.

### Steps to Run

1.  **Clone the Repository:**
    ```bash
    git clone <your-github-repository-url>
    cd Intelligent_Monitor_System
    # If using Git LFS, pull the large model and video files
    # git lfs pull
    ```

2.  **Place Required Files:**
    *   Ensure your trained `best.pt` model is inside the **`models/`** folder.
    *   Ensure your full-length `dispatch_video.mp4` file is in the **project's root directory**.

3.  **Build and Run with Docker Compose:**
    This single command builds the Docker image and runs the video processing script.
    ```bash
    docker compose up --build
    ```
    - The `--build` flag is only needed the first time. For subsequent runs, use `docker compose up`.
    - The script will execute inside the container, and you will see progress logs in your terminal.

4.  **Find the Result:**
    Once the script finishes, the container will stop. A new file named **`output_video.mp4`** will be created in your project's root directory.

---

## Development & Testing (Local Environment)

For development and debugging, you can run the project directly on your local Ubuntu machine.

### Local Setup

1.  **Setup Environment:**
    ```bash
    # Create and activate a Python virtual environment
    python3 -m venv .venv
    source .venv/bin/activate

    # Install dependencies
    pip install -r requirements.txt
    ```

### Choose a Testing Method:

#### Method A: Interactive Web UI (Recommended for Visualization)

This method launches a web application to view the model's performance in real-time.

1.  **Run the Streamlit App:**
    ```bash
    streamlit run src/app.py
    ```

2.  **View in Browser:** Open the local URL provided in the terminal (usually `http://localhost:8501`).

#### Method B: Fast Batch Processing Test

This method is for quickly testing the end-to-end processing pipeline on a smaller video clip.

1.  **Create a Test Clip:**
    First, run the splitting script to create a 10-minute test video.
    ```bash
    python split.py
    ```
    This will create a `test_clip.mp4` file in your project root.

2.  **Run the Local Test Script:**
    This script processes the shorter `test_clip.mp4`.
    ```bash
    python test.py
    ```
    The output will be saved as `output_video_local_test.mp4`.
---
## Advanced Optimization

The `optimize_threshold_rl.py` script demonstrates using a Q-Learning agent to find the optimal confidence threshold for the model. It can be run locally after setting up the environment.
```bash
python optimize_threshold_rl.py
```